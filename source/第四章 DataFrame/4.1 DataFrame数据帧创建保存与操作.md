# 4.1 DataFrame数据帧创建保存与操作(python版）

## 4.1.1 理论基础
### Session
SparkSession是使用DataFrame和DataSet API的入口点。
- 在Spark 1.0中用户需要通过SparkContext来创建和操作RDD(弹性分布式数据集),而对于其他功能如SQL处理和Hive集成,则需要使用不同的上下文(如SQLContext和HiveContext)。
- 随着DataFrame和DataSet API的普及,Spark 2.0引入了SparkSession作为统一的入口点,以便更方便地使用这些新API。
- SparkSession的作用包括提供统一的接口来创建和操作DataFrame和DataSet,同时它内部封装了SparkContext,这意味着实际的计算任务仍然是由SparkContext完成的。这使得用户可以更简单地使用Spark的各种功能,而不需要分别创建和管理多个上下文。
- 与旧版本Spark1.0的区别:在Spark1.0中用户需要先创建SparkConf 和SparkContext,然后才能进行后续的操作；而Spark 2.0及以后的版本中,通过SparkSession可以更简单地实现相同的功能。

### SparkSession接口 & DataFrame
- 从Spark2.0以上版本开始,Spark使用全新的SparkSession接口替代Spark1.6中的SQLContext及HiveContext 接口来实现其对数据加载、转换、处理等功能。
- SparkSession实现了SQLContext及HiveContext 所有功能。
- SparkSession支持从不同的数据源加载数据,并把数据转换成DataFrame数据帧,并且支持把DataFrame 转换成SQLContext自身中的表,然后使用SQL语句来操作数据。SparkSession亦提供了HiveQL以及其他依赖于Hive的功能的支持。

### DataFrame和RDD的区别：
- RDD是分布式的Java对象的集合,比如,RDD[Person]是以Person为类型参数,但是,Person类的内部结构对于RDD而言却是不可知的。
- DataFrame是一种以RDD为基础的分布式数据集,也就是分布式的Row对象的集合(每个Row对象代表一行记录),提供了详细的结构信息,也就是我们经常说的模式(schema),Spark SQL可以清楚地知道该数据集中包含哪些列、每列的名称和类型。

与RDD一样,DataFrame的各种变换操作也采用惰性机制,只是记录了各种转换的逻辑转换路线图(是一个DAG有向无环图),不会发生真正的计算

## 4.1.2 集群启动
在3台服务器分别启动Zookeeper集群服务器

```bash
zkServer.sh start
```

在hadoop01主服务器上开启集群

```bash
start-dfs.sh
start-yarn.sh
start-spark-all.sh
```

## 4.1.3 创建DataFrame
使用SparkSession来创建DataFrame

### Step 1:数据准备
进入hadoop01虚拟机的linux-shell,在`/export/servers/spark/example/src/main/resources/`中找到样例数据`people.json`和`people.txt`

### Step 2:生成DataFrame并显示数据
以下操作将从people.json文件中读取数据并生成DataFrame数据帧并显示数据

在hadoop01中启动`pyspark`进入spark集群模式下的python-shell交互式命令行环境

创建一个pyspark提供的SparkSession对象spark（DataFrame数据帧对象spark）

```bash
>>> spark = SparkSession.builder.getOrCreate()
>>> df = spark.read.json("file:///export/servers/spark/examples/src/main/resources/people.json")
>>> df.show()
```

<img 

## 4.1.4 保存DataFrame
可以使用`spark.write`操作,把一个DataFrame保存成不同格式的文件。

eg.把一个名称为`df`的DataFrame保存到不同格式文件中

```bash
df.write.text("people.txt")				
# 保存到.txt文本文件中
df.write.json("people.json")			
# 保存到.json轻量级数据交换文件中
df.write.parquet("people.parquet")	
# 保存到.parquet列式存储格式文件中
```

或

```bash
df.write.format("text").save("people.txt")   
# 保存到.txt文本文件中
df.write.format("json").save("people.json")  
# 保存到.json轻量级数据交换文件中
df.write.format("parquet").save("people.parquet")
# 保存到.parquet 列式存储格式文件中
```

### Step 1:DataFrame保存
从示例文件`people.json`中创建一个DataFrame,名称为`peopleDF`。

把`peopleDF`保存到另外一个JSON文件中。

再从`peopleDF`中选取一个列(即name列),把该列数据保存到一个文本文件中。

```bash
>>> peopleDF = spark.read.format("json").load("file:///export/servers/spark/examples/src/main/resources/people.json")
>>> peopleDF.select("name","age").write.format("json").save("file:///export/servers/spark/examples/src/main/resources/newpeople.json")
>>> peopleDF.select("name").write.format("text").save("file:///export/servers/spark/examples/src/main/resources/newpeople.txt")
```

### Step 2:查看结果
克隆一个hadoop01的虚拟机，在克隆机中查询执行结果

<img 

<img 

## 4.1.5 打开DataFrame结果文件
如果要再次读取`newpeople.json`文件目录（此时的newpeople.json是一个文件目录，并不是真正的.json文件）中数据生成DataFrame,可以直接使用`newpeople.json`目录名称,而不需要使用`part-00000-3db90180-ec7c-4291-ad05-df8e45c77f4d.json`(使用也可以)。

在hadoop01的pyspark中执行以下命令

```bash
>>> peopleDF = spark.read.format("json").load("file:///export/servers/spark/examples/src/main/resources/newpeople.json")
>>> peopleDF.show()
```

<img

## 4.1.6 常用的DataFrame操作
在hadoop01中启动`pyspark`

1. 打印模式信息`printSchema()`
<img

2. 选择多列`select(...)`
<img 

3. 条件过滤`filter(...)`
<img

4. 分组聚合`groupBy(...)`
<img

5. 排序（逆序：desc；正序：asc）`sort(...)`
<img 

6.多列排序
<img 

对年龄进行逆序排序，对名字进行正序排序

7. 对列进行重命名`alias(...)`
<img

本章完！

最后`exit()`退出pyspark-shell，关闭集群！

