# 2.5 RDD文件数据读写（Python版）

## 2.5.0 准备工作
本地数据源准备，`/export/data/spark/mycode/python/rdd`目录下的`word.txt`（在2.1中已创建）

```bash
# word.txt
Hadoop is good,
Spark is fast,
Spark is better
```

在主节点界面中开启集群

```bash
start-dfs. sh
start-yarn. sh
start-spark-all.sh
```

使用`jps`进程查询命令时，01的6个进程,02的5个进程,03的4个进程都缺一不可

## 2.5.1 本地文件系统的数据读写

### Step 1:从文件中读取数据
在`pyspark`中执行命令：

```bash
>>> textFile = sc.textFile("file:///export/data/spark/mycode/python/rdd/word.txt")
>>> textFile.first()
```

<img src=

执行第一句时并不会马上显示结果，这是因为spark采用了**惰性机制**，执行“行动”类型的操作`.first()`查看

正是spark采用了惰性机制，在执行转换操作的时候，即使输入了错误的语句，pyspark也不会报错，而是等到执行“行动”类型的语句启动真正的计算时，才会报错

### Step 2:写入另外的文本中
在01中，将`textFile`变量中的内容再次写到另外一个文本文件`wordback.txt`中

```bash
>>> textFile = sc.textFile("file:///export/data/spark/mycode/python/rdd/word.txt")
>>> textFile.saveAsTextFile("file:///export/data/spark/mycode/python/rdd/writeback.txt")
```

`saveAsTextFile()`是一个行动类型的操作，马上会执行真正计算过的结果，从`word.txt`中加载数据到变量`textFile`中，然后又把`textFile`中的写回到`writeback.txt`中。

### Step 3:查看新写入文件
再克隆一个01会话，在克隆端的linux下查看`wordback.txt`。

```bash
cd /export/data/spark/mycode/python/rdd/writeback.txt
ll -a
```

<img src

系统会自动为`textFile`这个RDD生成两个分区，即`part-00000`和`part-00001`


在01克隆端，查看分区文件

```bash
cat part-00000
cat part-00001
```

<img

文件`part-00000`和`part-00001`中存储的内容与`word.txt`相同。

切换回01主服务器`pyspark`终端命令行界面，再次把数据加载到RDD中，只要使用`writeback.txt`目录即可

<img src=

## 2.5.2 分布式文件系统HDFS的数据读写
### Step 1:创建用户目录
在01主服务器的linux界面下，为当前linux登录用户创建目录，使用用户名root登录Linux系统终端

```bash
hdfs dfs -mkdir -p /user/root
hdfs dfs -ls .
hdfs dfs -ls /user/root	//方式二：采用HDFS的绝对路径
```

`/user/用户名`创建的目录在HDFS文件系统中，不在本地文件系统中

### Step 2:查看文件系统目录下的内容
在linux-shell中查看HDFS文件系统根目录下的内容

```bash
hadoop dfs -ls
# 方式1：hadoop dfs
hdfs dfs -ls
# 方式2：hdfs dfs
```

<img src=

### Step 3:浏览器查看
在浏览器中查看HDFS文件系统根目录下的内容。

在地址栏输入`http://hadoop01:50070`或`http://主节点IP:50070`，选择`Utilities`—>`Browse the file system`查看分布式文件系统里的数据文件

<img src=

### Step 4:新增文本文件
在01克隆端的Linux-shell界面下，将本地文件系统中的`/export/data/spark/mycode/python/rdd/word2.txt`上传到分布式文件系统HDFS中（放到root用户目录下）

`word2.txt`可由`word.txt`复制而得

```bash
cd /export/data/spark/mycode/python/rdd/
cp word.txt word2.txt
hdfs dfs -put /export/data/spark/mycode/python/rdd/word2.txt
```

刷新浏览器HDFS的UI网页，查看是否新增`word2.txt`

<img src=

在01克隆端采用linux-shell命令查看root用户目录下是否多了word2.txt文件

<img src=

切换到01主服务器的`pyspark`会话，编写语句从HDFS中加载word2.txt文件，并显示第一行文本内容

```bash
>>> textFile = sc.textFile("hdfs://01的IP:9000/user/root/word2.txt")
>>> textFile.first()
```

这里的`sc.textFile("hdfs://01的IP:9000/user/root/word2.txt")`也等价于以下两条语句

```bash
>>> textFile = sc.textFile("/user/root/word2.txt")                
>>> textFile = sc.textFile("word2.txt")
```

### Step 5:写入文件系统中
把`textFile`的内容写回到HDFS文件系统中(写到root用户目录下),在01主服务器终端的`Pyspark-she11`界面执行

```bash
>>> textFile = sc.textFile("word2.txt")
>>> textFile.saveAsTextFile("writeback")
```
执行以上命令后,文本内容会被写入到HDFS文件系统的`/user/hadoop/writeback.txt`目录下

我们可以切换到01克隆端的Linux Shell命令提示符窗口查看

```bash
hdfs dfs -ls .
hdfs dfs -ls ./writeback
hdfs dfs -cat ./writeback/part-00000
hdfs dfs -cat ./writeback/part-00001
```

<img src=

刷新浏览器HDFS的UI网页，得到新增文件`writeback`

当需要再次把`writeback.txt`中的内容加载到RDD中时,只需要加载**`writeback.txt`目录**即可，不需要使用part-00000 文件.

## 2.5.3 不同文件格式的读写模式
### 1.文本文件
将本地文件中的文本文件加载到RDD中的语句

```bash
>>> rdd = sc.textFile("file:///export/data/spark/mycode/python/rdd/word.txt")
>>> rdd.saveAsTextFile("hdfs://192.168.177.182:9000/user/root/writeback2.txt")
# 方式一
>>> rdd.saveAsTextFile("writeback2.txt")
# 方式二
```

当我们给`textFile()`函数传递一个**包含完整路径的文件名**时,就会把这个文件加载到RDD中。如果我们给textFile(函数传递的不是文件名,而是一个目录,则该目录下的所有文件内容都会被读取到RDD中。

方式一和方式二效果相同，在`saveAsTextFile()`函数的参数中给出的是目录,不是文件名,RDD中的数据会被保存到给定的目录下。

### 2.JSON文件
JSON是一种轻量级的数据交换格式,Spark提供了一个JSON样例数据文件,存放在`/export/servers/spark/examples/src/main/resources/people.json`

#### 案例：JSON数据文件处理
存放在`/export/servers/spark/examples/src/main/resources/people.json`中`people.json`文件的内容如下:

<img src=


