# 2.1 RDD创建（Python版）

## 2.1.1 准备工作
3个集群启动+数据源创建

#### Step 1：集群启动
先后启动Hadoop集群、Yarn集群、Spark集群

```bash
start-dfs. sh
start-yarn. sh
start-spark-all. sh
jps
# jps进程查询
```

01的6个进程,02的5个进程,03的4个进程都缺一不可

进入Pyspark:

在Master主服务器01中,启动pyspark

```bash
pyspark
```

#### Step 2：创建数据源文件
克隆一个01服务器的会话，在这个克隆端内创建数据源

效果如下：

<img src="">

在`/export/data/spark/mycode/python`目录下新建rdd子目录,用来存放本章的代码和相关文件

```bash
cd /export/data/spark/mycode/python
mkdir rdd	//创建子目录rdd
cd ./rdd
```

在rdd目录下新建一个word.txt文件。
```bash
vi word.txt
```

你可以在word.txt文件里面随便输入几行英文语句用来测试。

```bash
Hadoop is good,
Spark is fast,
Spark is better
```

## 2.1.2 创建RDD
**创建RDD有两种方式**：从本地文件系统加载数据 或 从HDFS中加载数据

### 本地文件系统加载数据：
Spark采用textFile()方法来从文件系统中加载数据创建RDD,该方法把文件的URI作为参数,这个URI可以是本地文件系统的地址,或者是分布式文件系统HDFS的地址,或者是Amazon S3的地址等等。

成功创建本地数据源文件`/export/data/spark/mycode/python/rdd/word.txt`之后,回到01主服务器终端的`Pyspark`命令行界面(“>>>”),在`Pyspark`交互式命令行界面,逐行执行如下命令

```bash
>>> lines = sc.textFile("file:///export/data/spark/mycode/python/rdd/word.txt")
>>> lines.foreach(print)
#打印输出RDD中的每一个元素
```

打印执行效果如下图,实现了方式1:从本地文件系统中加载数据

<img src=>

### HDFS中加载数据
把刚才在本地文件系统中的“/export/data/spark/mycode/python/rdd/word.txt”上传到HDFS文件系统的root用户的指定目录下(首次采用wordCount/input普通目录)

**注意**:本教程统一使用root用户登录Linux系统

打开HDFS的UI网页：在地址栏输入http://hadoop01:50070(或输入http://01的IP:50070)选择Utilities → Browse the file system查看分布式文件系统里的数据文件，首次执行发现可以看到HDFS上没有任何数据文件

<img src=>

实现在HDFS分布式文件系统中创建/wordcount/input目录,并将上面的word.txt文件上传至该目录/wordcount/input中(该目录为不可见文件目录)，在**hadoop01(1)克隆终端**中执行以下命令

```bash
hadoop fs -mkdir -p /wordCount/input  
# 在HDFS分布式文件系统中创建目录/wordcount/input
hadoop fs -put /export/data/spark/mycode/python/rdd/word.txt /wordCount/input
# 将本地/export/data目录下的word.txt文件上传到HDFS文件系统的/wordcount/input 目录中。
```

hadoop fs 是Hadoop开发框架提供的进行文件系统操作的HDFS She11命令格式。

刷新HDFS的UI界面，可以看到word.txt文件已经成功上传到HDFS分布式文件系统中了。

<img src=>

在**hadoop01(1)克隆终端**中查看/wordCount/input/word.txt 源文件中内容:

```bash
hadoop fs -cat /wordCount/input/word.txt
```

<img src=>

从HDFS文件系统中加载数据有三种命令：(在pyspark中执行)
```bash
>>>lines=sc.textFile("hdfs://hadoop01:9000/user/root/word.txt")
# 方式1:HDFS分布式文件系统中默认文件存储路径
>>>lines=sc.textFile("/wordCount/input/word.txt")
# 方式2:HDFS分布式文件系统中,自定义文件存储路径
>>>lines=sc.textFile("word.txt")
# 方式3:HDFS分布式文件系统中默认文件存储路径
```

**注意**：

1.以上三条命令是完全等价的命令,只不过使用了不同的目录形式,你可以使用其中任意一条命令完成数据加载操作。方式1=方式3(都为默认文件存储路径)，方式2采用的是自定义文件存储路径

2.方式1和方式3会出现报错：
<img src=>

HDFS文件系统为Linux登录用户开辟的默认目录是`/user/用户名`(是`user`,不是`usr`),因为这里是采用root用户名登录Linux系统的,因此需创建的目录应该为`/user/root`,再次强调一下,该目录仅仅在HDFS文件系统中有效,本地文件系统中是无效的,在01克隆端中执行以下命令实现;

```bash
hdfs dfs -mkdir -p /user/root
# 创建与登录用户root向匹配的HDFS文件系统默认目录
```

刷新HDFS的UI界面，可查询到/user/root目录创建成功。

再将本地文件系统中的`/export/data/spark/mycode/python/rdd/word.txt`文件上传到HDFS文件系统的root用户的指定目录下(CentOS7系统强制要求采用/user/root目录),在01克隆端中实现命令如下

```bash
hadoop fs -put /export/data/spark/mycode/python/rdd/word.txt /user/root
```

<img src=>

最后，切换到hadoop01终端的Pyspark命令行界面,使用方式1和方式3完成从HDFS文件系统中加载数据

方式1
```bash
>>> lines =sc.textFile("hdfs://hadoop01:9000/user/root/word.txt")
>>> lines. foreach (print)
```

方式3:(方式3采用的就是默认/user/root目录)
```bash
>>> lines = sc.textFile("word.txt")
>>> lines. foreach (print)
```

## 2.1.3 通过并行集合(列表)创建RDD
可以调用SparkContext的parallelize方法,在Driver中一个已经存在的集合(数组)上创建。

在pyspark中执行

### 案例1

```bash
>>> nums = [1,2,3,4,5]
>>> rdd = sc.parallelize (nums)
>>> rdd.foreach (print)
```

<img src=>

通过集合(列表)创建无序RDD,成功!

### 案例2

```bash
>>> array = [1, 2, 3, 4, 5]
>>> rdd = sc. parallelize(array)
>>> rdd. foreach (print)
```

<img src=>

//通过数组创建有序RDD,成功!
