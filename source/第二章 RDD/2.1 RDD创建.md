# 2.1 RDD创建（Python版）

## 2.1.1 准备工作
3个集群启动+数据源创建

#### Step 1：集群启动
先后启动Hadoop集群、Yarn集群、Spark集群

```bash
start-dfs. sh
start-yarn. sh
start-spark-all. sh
jps
# jps进程查询
```

01的6个进程,02的5个进程,03的4个进程都缺一不可

进入Pyspark:

在Master主服务器01中,启动pyspark

```bash
pyspark
```

#### Step 2：创建数据源文件
克隆一个01服务器的会话，在这个克隆端内创建数据源

效果如下：

<img src="">

在`/export/data/spark/mycode/python`目录下新建rdd子目录,用来存放本章的代码和相关文件

```bash
cd /export/data/spark/mycode/python
mkdir rdd	//创建子目录rdd
cd ./rdd
```

在rdd目录下新建一个word.txt文件。
```bash
vi word.txt
```

你可以在文件里面随便输入几行英文语句用来测试。

```bash
Hadoop is good,
Spark is fast,
Spark is better
```

