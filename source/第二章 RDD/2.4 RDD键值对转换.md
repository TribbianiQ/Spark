# 2.4 RDD键值对转换

## 2.4.0 准备工作
集群启动

```bash
start-dfs. sh
start-yarn. sh
start-spark-all.sh
```

进入`pyspark`界面

```bash
pyspark
```

## 2.4.1 RDD创建
键值对RDD是指每个RDD元素都是(key,value)键值对类型，是一种常见的RDD类型。

键值对的创建主要有两种方式：从文件中加载生成RDD、通过并行集合（列名）

### 方式一
从文件中加载生成RDD

```bash
>>> lines = sc.textFile("file:///export/data/spark/mycode/python/rdd/word.txt")
>>> pairRDD = lines.flatMap(lambda line:line.split(" ")).map(lambda word:(word,1))
>>> pairRDD.foreach(print)
```

<img src=>

### 方式二
通过并行集合（列表）创建RDD

```bash
>>> list = ["Hadoop","Spark","Hive","Spark"]
>>> rdd = sc.parallelize(list)
>>> pairRDD = rdd.map(lambda word:(word,1))
>>> pairRDD.foreach(print)
```

<img src=>

## 2.4.2 RDD操作
常用的键值对转换：

`reduceByKey(func)`、`groupByKey()`、`keys`、`vaules`、`sortByKey()`、`sortByKey()`、`sortBy()`、`mapValues(func)`、`combineByKey()`、`join()`、`cogroup()`等

### 案例1：reduceByKey(func)
**合并(键值累加)**，使用func函数合并具有相同键的值。按key进行分组，然后将key相同的键值对的value都执行func操作，得到一个值。

```bash
>>> pariRDD = sc.parallelize([("Hadoop",1),("Spark",1),("Hive",1),("Spark",1)])
>>> pairRDD.reduceByKey(lambda a,b:a+b).foreach(print)
```

<img src=>

### 案例2：gruopByKey()
**分组(键值不累加，而是归类)**，`groupByKey()`的功能是对具有相同键的值进行分组。

```bash
>>> list = [("spark",1),("spark",2),("hadoop",3),("hadoop",5)]
>>> pairRDD = sc.parallelize(list)
>>> pairRDD.groupByKey()
>>> pairRDD.groupByKey().foreach(print)
```

<img src=>

groupByKey()会为每个Key生成一个value-list，每个value-list被保存为可迭代的ResultIterable对象

### 案例3：reduceByKey()&groupByKey()
`reduceByKey()`用于对每个Key对应的多个value进行聚合操作，并且聚合操作可以通过函数func进行自定义。

`groupByKey()`也是对每个key进行操作，对每个Key只会生成一个value-list，groupByKey()本身不能自定义函数，需要先用`groupByKey()`生成RDD，然后才能对此RDD通过map()进行自定义函数操作。

```bash
>>> words = ["one","two","two","three","three","three"]
>>> wordPairsRDD = sc.parallelize(words).map(lambda word:(word,1))
>>> wordCountsWithReduce = wordPairsRDD.reduceByKey(lambda a,b:a+b)
>>> wordCountsWithReduce.foreach(print)
```

<img src=>

可以得到`wordCountsWithReduce`和`wordCountsWithGroup`的结果是完全一样的，但是它们的内部运算过程是不同的

### 案例4：keys
键值对RDD每个元素都是(key,value)的形式，keys方法只会把键值对RDD中的key返回，形成一个新的RDD。

```bash
>>> list = [("Hadoop",1),("Spark",1),("Hive",1),("Spark",1)]
>>> pairRDD = sc.parallelize(list)
>>> pairRDD.keys().foreach(print)
```

<img src=>

### 案例5：values
Values操作只会把键值对RDD中的value返回，形成一个新的RDD。

```bash
>>> list = [("Hadoop",1),("Spark",1),("Hive",1),("Spark",1)]
>>> pairRDD = sc.parallelize(list)
>>> pairRDD.values().foreach(print)
```

<img src=>

### 案例6：sortByKey()
**重复不覆盖、不累加**，sortByKey()的功能是返回一个根据key排序的RDD。

```bash
>>> list = [("Hadoop",1),("Spark",1),("Hive",1),("Spark",1)]
>>> pairRDD = sc.parallelize(list)
>>> pairRDD.sortByKey().foreach(print)
```

<img src=>

### 案例7：mapValues(func)
mapValues(func)对键值对RDD中的每个value都应用一个函数，但是key不会发生变化。

```bash
>>> list = [("Hadoop",1),("Spark",1),("Hive",1),("Spark",1)]
>>> pairRDD = sc.parallelize(list)
>>> pairRDD1 = pairRDD.mapValues(lambda x:x+1)
>>> pairRDD1.foreach(print)
```

<img

映射=分解=检索=遍历

### 案例8：join()
join()表示内连接，对于给定的两个输入数据集(K,V1)和(K,V2)，只有在两个数据集中都存在的key才会被输出，最终得到一个(K(V1,V2))类型的数据集。

```bash
>>> pairRDD1 = sc.parallelize([("spark",1),("spark",2),("hadoop",3),("hadoop",5)])
>>> pairRDD2 = sc.parallelize([("spark","fast")])
>>> pairRDD3 = pairRDD1.join(pairRDD2)
>>> pairRDD3.foreach(print)
```

<img

pairRDD1和pairRDD2都有相同的key值`spark`，保留相同的key值，连接各自的value值。

### 综合实例
RDD键值对转换综合实例：给定一组键值对（“spark”,2）、（”Hadoop”,6）、（“Hadoop”,4）、（”spark”,6），键值对的key表示图书名称，value表示图书某天的销量，现在需要计算每个键对应的平均值，也就是计算每种图书每天的平均销量

```bash
>>> rdd = sc.parallelize([("spark",2),("hadoop",6),("hadoop",4),("spark",6)])
>>> rdd.mapValues(lambda x:(x,1)).reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1])).mapValues(lambda x:x[0]/x[1]).collect()
```

结果显示：`[('hadoop', 5.0), ('spark', 4.0)]`

## 2.4.3 共享变量
为了满足需要在多个任务之间共享变量,或者在任务(Task)和任务控制节点(Driver Program)之间共享变量的需求。
Spark提供了两种类型的变量:
- **广播变量**(broadcast variables)用来把变量在所有节点的内存之间进行共享。
- **累加器**(accumulators)则支持在所有不同节点之间进行累加计算(比如计数或者求和)。

### 广播变量
广播变量(broadcast variables)允许程序开发人员在每个机器上缓存*一个只读的变量*,而不是为机器上的每个任务都生成一个副本。

通过这种方式,就可以非常高效地给每个节点/机器提供一个大的输入数据集的副本。

Spark的“动作”操作会跨越多个阶段(stage),对于每个阶段内的所有任务所需要的公共数据,Spark都会自动进行广播。

通过广播方式进行传播的变量,会经过序列化,然后在被任务使用时再进行反序列化。这就意味着,显式地创建广播变量只有在下面的情形中是有用的:当跨越多个阶段的那些任务需要相同的数据,或者当以反序列化方式对数据进行缓存是非常重要的。

可以通过调用`SparkContext.broadcast(v)`来从一个普通变量v中创建一个广播变量。这个广播变量就是对普通变量v的一个包装器,通过调用value方法就可以获得这个广播变量的值。

```bash
>>> broadcastVar = sc.broadcast([1,2,3])
>>> broadcastVar.value
```

该广播变量被创建以后,那么在集群中的任何函数中,都应该使用广播变量broadcastVar的值,这样就不会把其他变量值重复分发到这些节点上。

此外,一旦广播变量创建后,普通变量v的值就不能再发生修改,从而确保所有节点都获得这个广播变量的相同的值。

### 累加器
累加器是仅仅被相关操作累加的变量,通常可以被用来实现**计数器(counter)** 和 **求和(sum)**。

Spark原生地支持数值型(numeric)的累加器,程序开发人员可以编写对新类型的支持。如果创建累加器时指定了名字,则可以在Spark UI界面看到,这有利于理解每个执行阶段的进程。

一个数值型的累加器,可以通过调用`SparkContext.accumulator()`来创建。运行在集群中的任务,就可以使用add方法来把数值累加到累加器上,但是,这些任务只能做累加操作,不能读取累加器的值,只有任务控制节点可以使用value方法来读取累加器的值。

```bash
>>> accum = sc.accumulator(0)
>>> sc.parallelize([1,2,3,4]).foreach(lambda x:accum.add(x))
>>> accum.value
```

<img src

使用累加器来对一个数组中的元素进行求和
