# 3.3 HBase与Spark交互操作(Python版)

HBase则是基于**列**进行数据的存储,HBase是一个高可靠性、高性能、面向列、可伸缩的分布式数据库。

这样的话,HBase就可以随着存储数据的不断增加而实时动态的增加列,从而满足Spark计算框架可以实时的将处理好的数据存储到HBase数据库中的需求。

## 3.3.1 集群启动

启动5个集群（Zookeeper分布式协调服务集群、HDFS集群、Yarn集群、Spark集群、HBase集群）

```bash
[root@hadoop01/02/03 ~]# zkServer.sh start
[root@hadoop01 ~]# start-dfs.sh
[root@hadoop01 ~]# start-yarn.sh
[root@hadoop01 ~]# start-spark-all.sh
[root@hadoop01/02/03 ~]# ntpdate -u cn.pool.ntp.org
[root@hadoop01 ~]# start-hbase.sh
```

使用`jps`命令查看进程服务，得到：主节点01:HMaster进程；从节点02:HRegionServer进程；从节点03:HRegionServer进程

## 3.3.2 进入HBase交互界面
在01主服务器启动进入HBase-shell命令行界面

```bash
hbase shell
```

<img 

进入HBase-shell交互命令界面之后，即可进行HBase数据库操作（增删改查）

## 3.3.3 HBase-shell基本操作
- `create`: 创建表
- `put`: 插入或更新数据
- `scan`: 扫描表并返回表的所有数据
- `describe`: 查看表的结构
- `get`: 获取指定行或列的数据
- `count`: 统计表中数据的行数
- `delete`: 删除指定行或者列的数据
- `deleteall`: 删除整个行或列的数据
- `truncate`: 删除整个表中的数据,但结构还在
- `drop`: 删除整个表,数据和结构都删除(慎用)

### list查询表
list命令可以查明当前HBase数据库中有哪些已经创建好的表

<img 

HBase自带数据流Table，建议保留

### create创建表
语法格式为`create 'table name','column family'`，其中table name为表名，column family为列族名。

注意：在关系型数据库MYSQL中，需要首先创建数据库，然后再创建表。但是**在HBase数据库中，不需要创建数据库，只要直接创建表就可以。**

<img 

### put操作
**插入**或者**更新**表中的数据。语法格式为`put 'table name','rowl','column family:column name','value'`

其中：table name表名，rowl行键（Row Key），column family列族名，column name列名，value列值

```bash
hbase(main)> put 'student','1001','info:sex','male'
hbase(main)> put 'student','1001','info:age','18'
hbase(main)> put 'student','1002','info:name','laochen'
hbase(main)> put 'student','1002','info:sex','female'
hbase(main)> put 'student','1002','info:age','20'
```

在例子中，student为表名，1001/1002为行标，info列族名，sex/age/name列名，female/male/18/20列值

更新 student表中行键为1001，列名info:age，键值18这一条数据中的值更新为100

```bash
hbase(main)> put 'student','1001','info:age','100'
```

<img 

### scan扫描
scan扫描student表中的所有数据

```bash
hbase(main)> scan 'student'
```

### describe查看表
describe查看表结构，语法格式为`describe 'table name'`

<img 

输出了student表的结构，表结构包含很多字段信息：
- name:表示列族名
- bloomfilter:表示为列族级别的类型
- verions:表示版本数
- in_memory:设置是否存入内存
- keep_deleted_cells:设置被删除的数据，在基于时间的历史数据查询中是否依然可见
- data_block_encoding:表示数据块的算法
- ttl:表示版本存活时间
- compression:表示设置压缩算法
- min_versions:表示最小版本数
- blockcache:表示是否设置读缓存
- replication_scope:表示设置备份

### get获取
获取指定字段的操作。语法格式为`get 'table name','row'`

<img 

### count统计
统计操作。语法格式为`count 'table name'`

<img 

### delete/deleteall删除
删除一条数据`detele`；删除所有数据`deteleall`

<img 

<img 

### truncate清空
清空操作，清空表中所有的数据。语法格式为`truncate 'table name'`

<img 

清空student表操作，**仅仅清除了其中的内容，但是student表还在**

### disable禁用 & drop删除
禁用并删除表（**如果不先禁用表，则无法删除表**）

<img 

## 3.3.4 配置jar包
由于在Spark2.0以上版本(这里采用的是spark-2.4.0版本)中,缺少将HBase数据转换为Python可读数据的.jar包文件,因此在搭建环境时,就需要将HBase的lib目录下的一些jar文件拷贝到Spark中。
这些都是编程时需要引入的jar包,需要拷贝的jar文件包括:【1】所有hbase开头的jar文件、【2】`guava-12.0.1.jar`、【3】`htrace-core-3.1.0-incubating.jar`、【4】`protobuf-java-2.5.0.jar`。










